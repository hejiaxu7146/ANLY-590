{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = pd.read_csv('train.csv',sep='\\t')\n",
    "#test = pd.read_csv('test.csv',sep='\\t')\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=True ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 6. WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(x) for x in words]\n",
    "    words =  lemmatized_words  \n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "#nltk.download()   \n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=True ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\" )\n",
    "for review in train['reviews']:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17829\n",
      "['born', 'thunderstorm', 'grew', 'overnight', 'played', 'alone', 'played', 'survived', 'hey', 'wanted', 'everything', 'never', 'like', 'love', 'come', 'light', 'wore', 'envy', 'hated', 'survived', 'one', 'way', 'ticket', 'place', 'demon', 'go', 'wind', 'change', 'nothing', 'ground', 'ever', 'grow', 'hope', 'lie', 'taught', 'cry', 'pillow', 'survived', 'still', 'breathing', 'still', 'breathing', 'still', 'breathing', 'still', 'breathing', 'alive', 'alive', 'alive', 'alive', 'found', 'solace', 'strangest', 'place', 'way', 'back', 'mind', 'saw', 'life', 'stranger', 'face', 'mine', 'one', 'way', 'ticket', 'place', 'demon', 'go', 'wind', 'change', 'nothing', 'ground', 'ever', 'grow', 'hope', 'lie', 'taught', 'cry', 'pillow', 'survived', 'still', 'breathing', 'still', 'breathing', 'still', 'breathing', 'still', 'breathing', 'alive', 'alive', 'alive', 'alive', 'took', 'still', 'breathing', 'took', 'still', 'breathing', 'took', 'still', 'breathing', 'took', 'still', 'breathing', 'took', 'still', 'breathing', 'took', 'still', 'breathing', 'took', 'still', 'breathing', 'took', 'still', 'breathing', 'made', 'every', 'single', 'mistake', 'could', 'ever', 'possibly', 'make', 'took', 'took', 'took', 'gave', 'never', 'noticed', 'pain', 'knew', 'wanted', 'went', 'got', 'thing', 'said', 'told', 'would', 'never', 'forgotten', 'spite', 'still', 'breathing', 'still', 'breathing', 'still', 'breathing', 'still', 'breathing', 'alive', 'took', 'still', 'breathing', 'took', 'still', 'breathing', 'alive', 'took', 'still', 'breathing', 'took', 'still', 'breathing', 'alive', 'took', 'still', 'breathing', 'took', 'still', 'breathing', 'alive', 'alive', 'alive', 'alive', 'alive']\n",
      "['lyric']\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences) ) #795538\n",
    "print(sentences[0])\n",
    "'''\n",
    "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n",
    "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n",
    "'''\n",
    "print(sentences[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-02 18:23:51,083 : INFO : collecting all words and their counts\n",
      "2018-12-02 18:23:51,083 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-12-02 18:23:51,155 : INFO : PROGRESS: at sentence #10000, processed 353912 words, keeping 14995 word types\n",
      "2018-12-02 18:23:51,206 : INFO : collected 19973 word types from a corpus of 654147 raw words and 17829 sentences\n",
      "2018-12-02 18:23:51,210 : INFO : Loading a fresh vocabulary\n",
      "2018-12-02 18:23:51,218 : INFO : effective_min_count=40 retains 1783 unique words (8% of original 19973, drops 18190)\n",
      "2018-12-02 18:23:51,222 : INFO : effective_min_count=40 leaves 561833 word corpus (85% of original 654147, drops 92314)\n",
      "2018-12-02 18:23:51,228 : INFO : deleting the raw counts dictionary of 19973 items\n",
      "2018-12-02 18:23:51,228 : INFO : sample=0.001 downsamples 65 most-common words\n",
      "2018-12-02 18:23:51,228 : INFO : downsampling leaves estimated 463671 word corpus (82.5% of prior 561833)\n",
      "2018-12-02 18:23:51,236 : INFO : estimated required memory for 1783 words and 300 dimensions: 5170700 bytes\n",
      "2018-12-02 18:23:51,240 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-02 18:23:51,284 : INFO : training model with 4 workers on 1783 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-12-02 18:23:51,662 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-02 18:23:51,662 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-02 18:23:51,682 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-02 18:23:51,682 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-02 18:23:51,682 : INFO : EPOCH - 1 : training on 654147 raw words (463741 effective words) took 0.4s, 1193664 effective words/s\n",
      "2018-12-02 18:23:52,101 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-02 18:23:52,115 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-02 18:23:52,119 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-02 18:23:52,125 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-02 18:23:52,126 : INFO : EPOCH - 2 : training on 654147 raw words (463576 effective words) took 0.4s, 1064739 effective words/s\n",
      "2018-12-02 18:23:52,519 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-02 18:23:52,534 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-02 18:23:52,544 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-02 18:23:52,548 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-02 18:23:52,548 : INFO : EPOCH - 3 : training on 654147 raw words (463782 effective words) took 0.4s, 1107851 effective words/s\n",
      "2018-12-02 18:23:52,944 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-02 18:23:52,944 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-02 18:23:52,968 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-02 18:23:52,972 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-02 18:23:52,973 : INFO : EPOCH - 4 : training on 654147 raw words (463719 effective words) took 0.4s, 1111747 effective words/s\n",
      "2018-12-02 18:23:53,335 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-02 18:23:53,351 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-02 18:23:53,362 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-02 18:23:53,362 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-02 18:23:53,366 : INFO : EPOCH - 5 : training on 654147 raw words (463815 effective words) took 0.4s, 1194501 effective words/s\n",
      "2018-12-02 18:23:53,366 : INFO : training on a 3270735 raw words (2318633 effective words) took 2.1s, 1117455 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\" )\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-02 18:23:58,932 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-12-02 18:23:58,947 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2018-12-02 18:23:58,947 : INFO : not storing attribute vectors_norm\n",
      "2018-12-02 18:23:58,947 : INFO : not storing attribute cum_table\n",
      "2018-12-02 18:23:59,010 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-02 18:24:01,557 : INFO : loading Word2Vec object from 300features_40minwords_10context\n",
      "2018-12-02 18:24:01,588 : INFO : loading wv recursively from 300features_40minwords_10context.wv.* with mmap=None\n",
      "2018-12-02 18:24:01,588 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-12-02 18:24:01,588 : INFO : loading vocabulary recursively from 300features_40minwords_10context.vocabulary.* with mmap=None\n",
      "2018-12-02 18:24:01,588 : INFO : loading trainables recursively from 300features_40minwords_10context.trainables.* with mmap=None\n",
      "2018-12-02 18:24:01,588 : INFO : setting ignored attribute cum_table to None\n",
      "2018-12-02 18:24:01,588 : INFO : loaded 300features_40minwords_10context\n",
      "C:\\Users\\hejia\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\hejia\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \n",
      "C:\\Users\\hejia\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\hejia\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec \n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")\n",
    "#\n",
    "type(model.wv.syn0)\n",
    "#numpy.ndarray\n",
    "model.wv.syn0.shape\n",
    "#(14961, 300)  total 15000 words\n",
    "################################################################\n",
    "model[\"flower\"]\n",
    "model[\"flower\"].shape\n",
    "#(300,) # Word vector dimensionality \n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Words To Paragraphs, Attempt 1: Vector Averaging\n",
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\") # (300,) 0\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)  #len(model.wv.index2word):14961\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        counter =int(counter)\n",
    "        #\n",
    "        # Print a status message every 1000th review\n",
    "       \n",
    "        if counter%1000. == 0.:\n",
    "            print(\"Review %d of %d\" % (counter, len(reviews)) )\n",
    "        # \n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "        #\n",
    "        # Increment the counter\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sheet',\n",
       " 'deny',\n",
       " 'nasty',\n",
       " 'damned',\n",
       " 'tongue',\n",
       " 'worse',\n",
       " 'else',\n",
       " 'gonna',\n",
       " 'hotel',\n",
       " 'stopping',\n",
       " 'law',\n",
       " 'livin',\n",
       " 'track',\n",
       " 'gucci',\n",
       " 'michael',\n",
       " 'hang',\n",
       " 'bury',\n",
       " 'high',\n",
       " 'dust',\n",
       " 'girlfriend',\n",
       " 'alabama',\n",
       " 'killing',\n",
       " 'want',\n",
       " 'motherfuckin',\n",
       " 'hush',\n",
       " 'boat',\n",
       " 'silence',\n",
       " 'bend',\n",
       " 'hollywood',\n",
       " 'hour',\n",
       " 'le',\n",
       " 'freedom',\n",
       " 'bam',\n",
       " 'must',\n",
       " 'ghost',\n",
       " 'happening',\n",
       " 'kissing',\n",
       " 'alone',\n",
       " 'ceiling',\n",
       " 'tick',\n",
       " 'ohh',\n",
       " 'flow',\n",
       " 'order',\n",
       " 'yup',\n",
       " 'blah',\n",
       " 'miss',\n",
       " 'ray',\n",
       " 'free',\n",
       " 'stone',\n",
       " 'puerto',\n",
       " 'mud',\n",
       " 'judge',\n",
       " 'lemon',\n",
       " 'drag',\n",
       " 'cover',\n",
       " 'packed',\n",
       " 'halo',\n",
       " 'green',\n",
       " 'smokin',\n",
       " 'skrt',\n",
       " 'leaf',\n",
       " 'cause',\n",
       " 'rapper',\n",
       " 'double',\n",
       " 'sea',\n",
       " 'quit',\n",
       " 'whip',\n",
       " 'table',\n",
       " 'as',\n",
       " 'mine',\n",
       " 'low',\n",
       " 'seat',\n",
       " 'carry',\n",
       " 'millionaire',\n",
       " 'burned',\n",
       " 'run',\n",
       " 'tonight',\n",
       " 'wrist',\n",
       " 'dare',\n",
       " 'almost',\n",
       " 'heat',\n",
       " 'big',\n",
       " 'lyric',\n",
       " 'standin',\n",
       " 'summer',\n",
       " 'wit',\n",
       " 'neighbor',\n",
       " 'weather',\n",
       " 'south',\n",
       " 'summertime',\n",
       " 'dime',\n",
       " 'toy',\n",
       " 'momma',\n",
       " 'motherfucking',\n",
       " 'self',\n",
       " 'rope',\n",
       " 'wild',\n",
       " 'extra',\n",
       " 'care',\n",
       " 'cooler',\n",
       " 'hip',\n",
       " 'spinnin',\n",
       " 'cop',\n",
       " 'fan',\n",
       " 'loved',\n",
       " 'mother',\n",
       " 'sun',\n",
       " 'scar',\n",
       " 'kept',\n",
       " 'thrill',\n",
       " 'answer',\n",
       " 'quiero',\n",
       " 'worry',\n",
       " 'jam',\n",
       " 'blessing',\n",
       " 'growing',\n",
       " 'motivate',\n",
       " 'tough',\n",
       " 'already',\n",
       " 'please',\n",
       " 'beating',\n",
       " 'wearing',\n",
       " 'sam',\n",
       " 'deserve',\n",
       " 'goddamn',\n",
       " 'goin',\n",
       " 'sitting',\n",
       " 'seen',\n",
       " 'shaking',\n",
       " 'glory',\n",
       " 'choose',\n",
       " 'un',\n",
       " 'talkin',\n",
       " 'wood',\n",
       " 'language',\n",
       " 'creep',\n",
       " 'nail',\n",
       " 'rock',\n",
       " 'swing',\n",
       " 'hangin',\n",
       " 'wobble',\n",
       " 'knee',\n",
       " 'artie',\n",
       " 'whiskey',\n",
       " 'headed',\n",
       " 'peek',\n",
       " 'spending',\n",
       " 'someone',\n",
       " 'hole',\n",
       " 'dance',\n",
       " 'asleep',\n",
       " 'boy',\n",
       " 'hall',\n",
       " 'ai',\n",
       " 'price',\n",
       " 'catch',\n",
       " 'probably',\n",
       " 'lately',\n",
       " 'news',\n",
       " 'learn',\n",
       " 'uma',\n",
       " 'telling',\n",
       " 'spot',\n",
       " 'gave',\n",
       " 'song',\n",
       " 'beast',\n",
       " 'sleep',\n",
       " 'chick',\n",
       " 'imagine',\n",
       " 'jean',\n",
       " 'touchin',\n",
       " 'emotion',\n",
       " 'quick',\n",
       " 'freeze',\n",
       " 'map',\n",
       " 'use',\n",
       " 'snap',\n",
       " 'partyin',\n",
       " 'mar',\n",
       " 'sink',\n",
       " 'good',\n",
       " 'unless',\n",
       " 'fifty',\n",
       " 'sleeping',\n",
       " 'rag',\n",
       " 'love',\n",
       " 'smile',\n",
       " 'blow',\n",
       " 'life',\n",
       " 'boo',\n",
       " 'pour',\n",
       " 'mix',\n",
       " 'gotti',\n",
       " 'wondering',\n",
       " 'bag',\n",
       " 'sex',\n",
       " 'reality',\n",
       " 'like',\n",
       " 'point',\n",
       " 'dum',\n",
       " 'difference',\n",
       " 'ring',\n",
       " 'wherever',\n",
       " 'rhyme',\n",
       " 'flat',\n",
       " 'teach',\n",
       " 'rising',\n",
       " 'lying',\n",
       " 'part',\n",
       " 'bout',\n",
       " 'weed',\n",
       " 'fade',\n",
       " 'realize',\n",
       " 'hop',\n",
       " 'guy',\n",
       " 'shorty',\n",
       " 'beginning',\n",
       " 'clap',\n",
       " 'holdin',\n",
       " 'pill',\n",
       " 'easy',\n",
       " 'gram',\n",
       " 'screaming',\n",
       " 'bringing',\n",
       " 'pocket',\n",
       " 'couple',\n",
       " 'crib',\n",
       " 'smooth',\n",
       " 'ya',\n",
       " 'yuh',\n",
       " 'puttin',\n",
       " 'al',\n",
       " 'dough',\n",
       " 'jerk',\n",
       " 'may',\n",
       " 'perry',\n",
       " 'tag',\n",
       " 'flip',\n",
       " 'toe',\n",
       " 'knowing',\n",
       " 'king',\n",
       " 'climb',\n",
       " 'getting',\n",
       " 'grass',\n",
       " 'pum',\n",
       " 'choice',\n",
       " 'realized',\n",
       " 'hella',\n",
       " 'faith',\n",
       " 'prayer',\n",
       " 'dawg',\n",
       " 'game',\n",
       " 'hangover',\n",
       " 'bos',\n",
       " 'alright',\n",
       " 'dig',\n",
       " 'chain',\n",
       " 'jumpin',\n",
       " 'jewelry',\n",
       " 'book',\n",
       " 'shut',\n",
       " 'coast',\n",
       " 'push',\n",
       " 'rihanna',\n",
       " 'fly',\n",
       " 'locked',\n",
       " 'grace',\n",
       " 'giving',\n",
       " 'tempo',\n",
       " 'number',\n",
       " 'fit',\n",
       " 'waking',\n",
       " 'bottom',\n",
       " 'explain',\n",
       " 'shirt',\n",
       " 'breaking',\n",
       " 'pound',\n",
       " 'attention',\n",
       " 'common',\n",
       " 'pretend',\n",
       " 'rough',\n",
       " 'rollie',\n",
       " 'shoulda',\n",
       " 'wrapped',\n",
       " 'nightmare',\n",
       " 'street',\n",
       " 'strange',\n",
       " 'picture',\n",
       " 'saturday',\n",
       " 'naked',\n",
       " 'zone',\n",
       " 'shy',\n",
       " 'mon',\n",
       " 'mom',\n",
       " 'end',\n",
       " 'mercy',\n",
       " 'humble',\n",
       " 'two',\n",
       " 'bottle',\n",
       " 'crash',\n",
       " 'holla',\n",
       " 'cream',\n",
       " 'brave',\n",
       " 'sugar',\n",
       " 'split',\n",
       " 'surprise',\n",
       " 'make',\n",
       " 'rumour',\n",
       " 'player',\n",
       " 'could',\n",
       " 'record',\n",
       " 'gettin',\n",
       " 'molly',\n",
       " 'caught',\n",
       " 'hatin',\n",
       " 'ordinary',\n",
       " 'cost',\n",
       " 'brand',\n",
       " 'winter',\n",
       " 'verse',\n",
       " 'oah',\n",
       " 'wow',\n",
       " 'touching',\n",
       " 'alcohol',\n",
       " 'lover',\n",
       " 'snow',\n",
       " 'maybach',\n",
       " 'doctor',\n",
       " 'park',\n",
       " 'k',\n",
       " 'smiling',\n",
       " 'dawn',\n",
       " 'pick',\n",
       " 'blind',\n",
       " 'told',\n",
       " 'moving',\n",
       " 'distance',\n",
       " 'screamin',\n",
       " 'waste',\n",
       " 'saying',\n",
       " 'faster',\n",
       " 'darling',\n",
       " 'clear',\n",
       " 'light',\n",
       " 'feel',\n",
       " 'hot',\n",
       " 'school',\n",
       " 'fire',\n",
       " 'waited',\n",
       " 'secret',\n",
       " 'woah',\n",
       " 'changed',\n",
       " 'showin',\n",
       " 'said',\n",
       " 'lady',\n",
       " 'christmas',\n",
       " 'sipping',\n",
       " 'boot',\n",
       " 'crack',\n",
       " 'acting',\n",
       " 'night',\n",
       " 'pussy',\n",
       " 'awesome',\n",
       " 'bodied',\n",
       " 'whole',\n",
       " 'cigarette',\n",
       " 'dreaming',\n",
       " 'beauty',\n",
       " 'last',\n",
       " 'plane',\n",
       " 'smell',\n",
       " 'power',\n",
       " 'serious',\n",
       " 'mac',\n",
       " 'wait',\n",
       " 'ba',\n",
       " 'superstar',\n",
       " 'jackson',\n",
       " 'bed',\n",
       " 'queen',\n",
       " 'cut',\n",
       " 'rachel',\n",
       " 'mma',\n",
       " 'bite',\n",
       " 'leaving',\n",
       " 'marry',\n",
       " 'heart',\n",
       " 'waiting',\n",
       " 'get',\n",
       " 'different',\n",
       " 'try',\n",
       " 'honest',\n",
       " 'famous',\n",
       " 'figure',\n",
       " 'e',\n",
       " 'ay',\n",
       " 'doin',\n",
       " 'never',\n",
       " 'fuckin',\n",
       " 'type',\n",
       " 'business',\n",
       " 'letter',\n",
       " 'cried',\n",
       " 'evil',\n",
       " 'handle',\n",
       " 'look',\n",
       " 'smoking',\n",
       " 'baddest',\n",
       " 'slip',\n",
       " 'golden',\n",
       " 'shit',\n",
       " 'lemme',\n",
       " 'liquor',\n",
       " 'upset',\n",
       " 'proof',\n",
       " 'rick',\n",
       " 'dirty',\n",
       " 'jet',\n",
       " 'room',\n",
       " 'wash',\n",
       " 'strong',\n",
       " 'losin',\n",
       " 'honey',\n",
       " 'wasting',\n",
       " 'list',\n",
       " 'chillin',\n",
       " 'wassup',\n",
       " 'bright',\n",
       " 'month',\n",
       " 'ocean',\n",
       " 'riding',\n",
       " 'noise',\n",
       " 'annie',\n",
       " 'tripping',\n",
       " 'heartbreak',\n",
       " 'loving',\n",
       " 'ten',\n",
       " 'circle',\n",
       " 'ow',\n",
       " 'drove',\n",
       " 'seein',\n",
       " 'cool',\n",
       " 'little',\n",
       " 'radar',\n",
       " 'text',\n",
       " 'john',\n",
       " 'dude',\n",
       " 'middle',\n",
       " 'fact',\n",
       " 'mission',\n",
       " 'sittin',\n",
       " 'dem',\n",
       " 'woke',\n",
       " 'store',\n",
       " 'yeah',\n",
       " 'spinning',\n",
       " 'video',\n",
       " 'shade',\n",
       " 'eye',\n",
       " 'worth',\n",
       " 'romance',\n",
       " 'church',\n",
       " 'funny',\n",
       " 'la',\n",
       " 'fed',\n",
       " 'shower',\n",
       " 'pool',\n",
       " 'james',\n",
       " 'supposed',\n",
       " 'close',\n",
       " 'thought',\n",
       " 'rolling',\n",
       " 'wear',\n",
       " 'fault',\n",
       " 'kurt',\n",
       " 'stayed',\n",
       " 'drown',\n",
       " 'searching',\n",
       " 'piece',\n",
       " 'boyfriend',\n",
       " 'know',\n",
       " 'patient',\n",
       " 'dat',\n",
       " 'lo',\n",
       " 'chair',\n",
       " 'evening',\n",
       " 'x',\n",
       " 'chase',\n",
       " 'cap',\n",
       " 'thing',\n",
       " 'wiggle',\n",
       " 'gas',\n",
       " 'beat',\n",
       " 'five',\n",
       " 'true',\n",
       " 'watch',\n",
       " 'put',\n",
       " 'called',\n",
       " 'hail',\n",
       " 'might',\n",
       " 'swear',\n",
       " 'busy',\n",
       " 'mary',\n",
       " 'dog',\n",
       " 'tryna',\n",
       " 'tight',\n",
       " 'regret',\n",
       " 'haha',\n",
       " 'mention',\n",
       " 'listening',\n",
       " 'respect',\n",
       " 'saw',\n",
       " 'kinda',\n",
       " 'perfect',\n",
       " 'loose',\n",
       " 'beach',\n",
       " 'stuff',\n",
       " 'gold',\n",
       " 'taking',\n",
       " 'weight',\n",
       " 'missed',\n",
       " 'knock',\n",
       " 'white',\n",
       " 'always',\n",
       " 'date',\n",
       " 'foot',\n",
       " 'ta',\n",
       " 'sorry',\n",
       " 'take',\n",
       " 'running',\n",
       " 'control',\n",
       " 'throw',\n",
       " 'speed',\n",
       " 'hit',\n",
       " 'bow',\n",
       " 'window',\n",
       " 'pipe',\n",
       " 'lifted',\n",
       " 'apologize',\n",
       " 'switch',\n",
       " 'clean',\n",
       " 'brighter',\n",
       " 'pride',\n",
       " 'away',\n",
       " 'melody',\n",
       " 'anyway',\n",
       " 'stage',\n",
       " 'road',\n",
       " 'sit',\n",
       " 'c',\n",
       " 'somethin',\n",
       " 'mall',\n",
       " 'week',\n",
       " 'none',\n",
       " 'bet',\n",
       " 'el',\n",
       " 'roof',\n",
       " 'needed',\n",
       " 'mediocre',\n",
       " 'mornin',\n",
       " 'tied',\n",
       " 'wet',\n",
       " 'missing',\n",
       " 'wonderin',\n",
       " 'eh',\n",
       " 'wake',\n",
       " 'safe',\n",
       " 'walked',\n",
       " 'drinkin',\n",
       " 'praise',\n",
       " 'tellin',\n",
       " 'brr',\n",
       " 'red',\n",
       " 'lame',\n",
       " 'ballin',\n",
       " 'work',\n",
       " 'son',\n",
       " 'falling',\n",
       " 'panda',\n",
       " 'kill',\n",
       " 'ear',\n",
       " 'undone',\n",
       " 'homies',\n",
       " 'candle',\n",
       " 'brought',\n",
       " 'quarter',\n",
       " 'job',\n",
       " 'ask',\n",
       " 'stanky',\n",
       " 'thick',\n",
       " 'speaker',\n",
       " 'dream',\n",
       " 'bullshit',\n",
       " 'askin',\n",
       " 'live',\n",
       " 'root',\n",
       " 'hair',\n",
       " 'single',\n",
       " 'treat',\n",
       " 'sense',\n",
       " 'forever',\n",
       " 'mirror',\n",
       " 'shout',\n",
       " 'decision',\n",
       " 'anything',\n",
       " 'playin',\n",
       " 'step',\n",
       " 'conversation',\n",
       " 'style',\n",
       " 'old',\n",
       " 'living',\n",
       " 'shoot',\n",
       " 'remember',\n",
       " 'paid',\n",
       " 'grip',\n",
       " 'soon',\n",
       " 'swagger',\n",
       " 'place',\n",
       " 'tune',\n",
       " 'waist',\n",
       " 'go',\n",
       " 'movie',\n",
       " 'spent',\n",
       " 'grab',\n",
       " 'lay',\n",
       " 'someday',\n",
       " 'along',\n",
       " 'saved',\n",
       " 'monster',\n",
       " 'babe',\n",
       " 'blame',\n",
       " 'sign',\n",
       " 'na',\n",
       " 'di',\n",
       " 'sell',\n",
       " 'aye',\n",
       " 'enemy',\n",
       " 'walkin',\n",
       " 'everybody',\n",
       " 'sail',\n",
       " 'everywhere',\n",
       " 'benz',\n",
       " 'looking',\n",
       " 'changing',\n",
       " 'tap',\n",
       " 'praying',\n",
       " 'tired',\n",
       " 'year',\n",
       " 'grow',\n",
       " 'world',\n",
       " 'walk',\n",
       " 'beautiful',\n",
       " 'mad',\n",
       " 'grey',\n",
       " 'mask',\n",
       " 'strike',\n",
       " 'suck',\n",
       " 'begging',\n",
       " 'hundred',\n",
       " 'codeine',\n",
       " 'rest',\n",
       " 'empty',\n",
       " 'page',\n",
       " 'took',\n",
       " 'every',\n",
       " 'eminem',\n",
       " 'hotter',\n",
       " 'walking',\n",
       " 'heaven',\n",
       " 'killin',\n",
       " 'view',\n",
       " 'sold',\n",
       " 'slowly',\n",
       " 'short',\n",
       " 'attitude',\n",
       " 'ooh',\n",
       " 'brings',\n",
       " 'ground',\n",
       " 'mouth',\n",
       " 'system',\n",
       " 'state',\n",
       " 'chasing',\n",
       " 'done',\n",
       " 'droppin',\n",
       " 'highway',\n",
       " 'party',\n",
       " 'maybe',\n",
       " 'upon',\n",
       " 'floor',\n",
       " 'trick',\n",
       " 'came',\n",
       " 'simple',\n",
       " 'ahead',\n",
       " 'least',\n",
       " 'near',\n",
       " 'home',\n",
       " 'bee',\n",
       " 'right',\n",
       " 'dropped',\n",
       " 'written',\n",
       " 'together',\n",
       " 'pimp',\n",
       " 'stack',\n",
       " 'puck',\n",
       " 'seven',\n",
       " 'squad',\n",
       " 'patek',\n",
       " 'rico',\n",
       " 'water',\n",
       " 'making',\n",
       " 'goon',\n",
       " 'broad',\n",
       " 'fill',\n",
       " 'pas',\n",
       " 'beer',\n",
       " 'fear',\n",
       " 'message',\n",
       " 'counting',\n",
       " 'build',\n",
       " 'una',\n",
       " 'tell',\n",
       " 'drop',\n",
       " 'si',\n",
       " 'pa',\n",
       " 'prolly',\n",
       " 'play',\n",
       " 'till',\n",
       " 'plan',\n",
       " 'side',\n",
       " 'voice',\n",
       " 'fool',\n",
       " 'money',\n",
       " 'butterfly',\n",
       " 'move',\n",
       " 'whenever',\n",
       " 'lose',\n",
       " 'va',\n",
       " 'sweeter',\n",
       " 'lost',\n",
       " 'nice',\n",
       " 'ball',\n",
       " 'sunshine',\n",
       " 'whoa',\n",
       " 'feed',\n",
       " 'steady',\n",
       " 'beggin',\n",
       " 'yet',\n",
       " 'da',\n",
       " 'taught',\n",
       " 'happen',\n",
       " 'call',\n",
       " 'crown',\n",
       " 'trigger',\n",
       " 'lightning',\n",
       " 'taken',\n",
       " 'insane',\n",
       " 'anymore',\n",
       " 'bumpin',\n",
       " 'breeze',\n",
       " 'angel',\n",
       " 'ship',\n",
       " 'h',\n",
       " 'country',\n",
       " 'til',\n",
       " 'flight',\n",
       " 'con',\n",
       " 'beside',\n",
       " 'actin',\n",
       " 'belt',\n",
       " 'outta',\n",
       " 'welcome',\n",
       " 'pack',\n",
       " 'bullet',\n",
       " 'winner',\n",
       " 'mr',\n",
       " 'hero',\n",
       " 'false',\n",
       " 'miami',\n",
       " 'bill',\n",
       " 'impossible',\n",
       " 'sip',\n",
       " 'darkest',\n",
       " 'float',\n",
       " 'bitter',\n",
       " 'hard',\n",
       " 'help',\n",
       " 'yellow',\n",
       " 'later',\n",
       " 'oh',\n",
       " 'ching',\n",
       " 'rari',\n",
       " 'long',\n",
       " 'hear',\n",
       " 'one',\n",
       " 'homie',\n",
       " 'tip',\n",
       " 'small',\n",
       " 'shoe',\n",
       " 'bro',\n",
       " 'thirsty',\n",
       " 'reach',\n",
       " 'filled',\n",
       " 'rise',\n",
       " 'pitbull',\n",
       " 'issue',\n",
       " 'hallelujah',\n",
       " 'often',\n",
       " 'quiet',\n",
       " 'excuse',\n",
       " 'chicken',\n",
       " 'pole',\n",
       " 'mm',\n",
       " 'late',\n",
       " 'around',\n",
       " 'shine',\n",
       " 'turning',\n",
       " 'next',\n",
       " 'g',\n",
       " 'wonder',\n",
       " 'car',\n",
       " 'wife',\n",
       " 'finna',\n",
       " 'main',\n",
       " 'passion',\n",
       " 'california',\n",
       " 'dumb',\n",
       " 'load',\n",
       " 'closed',\n",
       " 'story',\n",
       " 'smoke',\n",
       " 'spit',\n",
       " 'carter',\n",
       " 'flower',\n",
       " 'chest',\n",
       " 'laugh',\n",
       " 'drank',\n",
       " 'bae',\n",
       " 'trip',\n",
       " 'hoping',\n",
       " 'brother',\n",
       " 'jesus',\n",
       " 'battle',\n",
       " 'glow',\n",
       " 'american',\n",
       " 'change',\n",
       " 'thank',\n",
       " 'hope',\n",
       " 'would',\n",
       " 'greatest',\n",
       " 'york',\n",
       " 'tie',\n",
       " 'credit',\n",
       " 'burning',\n",
       " 'crime',\n",
       " 'see',\n",
       " 'stupid',\n",
       " 'day',\n",
       " 'su',\n",
       " 'knife',\n",
       " 'dre',\n",
       " 'sling',\n",
       " 'bass',\n",
       " 'peace',\n",
       " 'human',\n",
       " 'steal',\n",
       " 'die',\n",
       " 'seems',\n",
       " 'history',\n",
       " 'trust',\n",
       " 'weezy',\n",
       " 'twice',\n",
       " 'magic',\n",
       " 'ew',\n",
       " 'mile',\n",
       " 'blowin',\n",
       " 'jump',\n",
       " 'many',\n",
       " 'door',\n",
       " 'happiness',\n",
       " 'hill',\n",
       " 'built',\n",
       " 'spell',\n",
       " 'guitar',\n",
       " 'worried',\n",
       " 'shot',\n",
       " 'breathin',\n",
       " 'sometimes',\n",
       " 'shake',\n",
       " 'asked',\n",
       " 'mike',\n",
       " 'heard',\n",
       " 'yah',\n",
       " 'cowboy',\n",
       " 'bless',\n",
       " 'meet',\n",
       " 'six',\n",
       " 'sent',\n",
       " 'phone',\n",
       " 'war',\n",
       " 'listen',\n",
       " 'second',\n",
       " 'confused',\n",
       " 'everyday',\n",
       " 'clip',\n",
       " 'bum',\n",
       " 'underneath',\n",
       " 'throwing',\n",
       " 'lived',\n",
       " 'body',\n",
       " 'block',\n",
       " 'within',\n",
       " 'press',\n",
       " 'leg',\n",
       " 'coat',\n",
       " 'drippin',\n",
       " 'mind',\n",
       " 'revenge',\n",
       " 'danced',\n",
       " 'knew',\n",
       " 'stop',\n",
       " 'dab',\n",
       " 'gotta',\n",
       " 'coke',\n",
       " 'wicked',\n",
       " 'apart',\n",
       " 'booty',\n",
       " 'bell',\n",
       " 'yea',\n",
       " 'kissed',\n",
       " 'heel',\n",
       " 'lick',\n",
       " 'ice',\n",
       " 'skin',\n",
       " 'settle',\n",
       " 'watchin',\n",
       " 'starting',\n",
       " 'yesterday',\n",
       " 'stunt',\n",
       " 'devil',\n",
       " 'hitta',\n",
       " 'word',\n",
       " 'drug',\n",
       " 'grown',\n",
       " 'hopeless',\n",
       " 'hanging',\n",
       " 'closing',\n",
       " 'halfway',\n",
       " 'hiding',\n",
       " 'west',\n",
       " 'flame',\n",
       " 'tock',\n",
       " 'vibe',\n",
       " 'rain',\n",
       " 'hood',\n",
       " 'turned',\n",
       " 'lesson',\n",
       " 'lip',\n",
       " 'turnin',\n",
       " 'building',\n",
       " 'ur',\n",
       " 'santana',\n",
       " 'hol',\n",
       " 'horse',\n",
       " 'chopper',\n",
       " 'tall',\n",
       " 'bang',\n",
       " 'learned',\n",
       " 'pop',\n",
       " 'island',\n",
       " 'arm',\n",
       " 'talked',\n",
       " 'fella',\n",
       " 'rockstar',\n",
       " 'finn',\n",
       " 'far',\n",
       " 'letting',\n",
       " 'motherfucker',\n",
       " 'buzz',\n",
       " 'direction',\n",
       " 'wraith',\n",
       " 'god',\n",
       " 'sum',\n",
       " 'parking',\n",
       " 'bus',\n",
       " 'post',\n",
       " 'share',\n",
       " 'believe',\n",
       " 'bigger',\n",
       " 'luck',\n",
       " 'act',\n",
       " 'slow',\n",
       " 'daughter',\n",
       " 'mean',\n",
       " 'win',\n",
       " 'singing',\n",
       " 'threw',\n",
       " 'minute',\n",
       " 'new',\n",
       " 'name',\n",
       " 'sh',\n",
       " ...}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(model.wv.index2word) #len(set(model.wv.index2word)):14961"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LISTEN TO THIS TRACK BITCH!\\r\\r\\n(Roscoe Dash)\\r\\r\\nGirl the way you're movin'\\r\\r\\nGot me in a trance\\r\\r\\nDJ turn me up\\r\\r\\nLadies dis yo jam\\r\\r\\nI'ma sip Moscato\\r\\r\\nAnd you 'gon lose dem pants\\r\\r\\nThen I'ma throw this money\\r\\r\\nWhile you do it with no hands\\r\\r\\nGirl drop it to the flo'\\r\\r\\nI love the way yo booty go\\r\\r\\nAll I want to do is sit back\\r\\r\\nAnd watch you move\\r\\r\\nAnd I'll proceed to throw this cash\\r\\r\\n\\r\\r\\n(Waka Flocka Flame)\\r\\r\\n\\r\\r\\nAll that ass\\r\\r\\nIn yo jeans\\r\\r\\nCan Wale beet?\\r\\r\\nCan Roscoe skeet?\\r\\r\\nLong hair she don't care\\r\\r\\nWhen she walk she get stares\\r\\r\\nBrown skin or a yellow-bone\\r\\r\\nDJ this my favorite song\\r\\r\\nSo I'ma make it thunderstorm\\r\\r\\nFlood warning, Flocka yeah\\r\\r\\nBlowin' fuck it I don't care\\r\\r\\nDreads is flyin' everywhere\\r\\r\\nTap my partner Roscoe, like bruh\\r\\r\\nI'm drunk as hell, can't you tell?\\r\\r\\nThrew 70 bands, bet 50 stacks\\r\\r\\nOh fuckin' well\\r\\r\\nI'm tryna hit the hotel\\r\\r\\nWith 2 girls that swallow me\\r\\r\\nTake this dick while I'm swallow\\r\\r\\nMoscato got her freaky\\r\\r\\nHey you got me in a trance\\r\\r\\nPlease take off yo pants\\r\\r\\nPussy pop on her handstand\\r\\r\\nYou got me sweatin'\\r\\r\\nPlease pass me a fan, damn!\\r\\r\\n\\r\\r\\n(Roscoe Dash)\\r\\r\\nGirl the way you're movin'\\r\\r\\nGot me in a trance\\r\\r\\nDJ turn me up\\r\\r\\nLadies dis yo jam\\r\\r\\nI'ma sip Moscato\\r\\r\\nAnd you 'gon lose dem pants\\r\\r\\nThen I'ma throw this money\\r\\r\\nWhile you do it with no hands\\r\\r\\nGirl drop it to the flo'\\r\\r\\nI love the way yo booty go\\r\\r\\nAll I want to do is sit back\\r\\r\\nAnd watch you move\\r\\r\\nAnd I'll proceed to throw this cash\\r\\r\\n\\r\\r\\n(Wale)\\r\\r\\nShe said look ma no hands\\r\\r\\nShe said look ma no hands\\r\\r\\nAnd no darling, I don't dance\\r\\r\\nAnd, I'm with Roscoe, I'm with Waka\\r\\r\\nI think I deserve a chance\\r\\r\\nI'm a bad motha-fucka\\r\\r\\nGon' ask some mothafuckas\\r\\r\\nA young handsome mothafucka\\r\\r\\nI sling that wood\\r\\r\\nI just nunchuck 'em\\r\\r\\nAnd, who you wit\\r\\r\\nAnd, what's yo name\\r\\r\\nYou not hip boo, I'm Wale\\r\\r\\nAnd, that D.C. shit I rep all day\\r\\r\\nAnd, my eyes red 'cause of all that haze\\r\\r\\nDon't blow my high\\r\\r\\nLet me shine\\r\\r\\nDrumma on the beat\\r\\r\\nLet me take my time\\r\\r\\nNigga want beef we can take it outside\\r\\r\\nFight for what broad\\r\\r\\nThese hoes ain't mine\\r\\r\\nIs you out yo mind?\\r\\r\\nYou out yo league\\r\\r\\nI sweat no bitches\\r\\r\\nJust sweat out weaves\\r\\r\\nWear out tracks\\r\\r\\nLet me do my thing\\r\\r\\nI got 16, for this Roscoe thing\\r\\r\\nBut, I'm almost done\\r\\r\\nLet me get back to it\\r\\r\\nWhole lotta loud\\r\\r\\nAnd a little backwood\\r\\r\\nWhole lotta money\\r\\r\\nBig tip I would\\r\\r\\nI put her on the train\\r\\r\\nLittle engine could, bitch\\r\\r\\n\\r\\r\\n(Roscoe Dash)\\r\\r\\nGirl the way you're movin'\\r\\r\\nGot me in a trance\\r\\r\\nDJ turn me up\\r\\r\\nLadies dis yo jam\\r\\r\\nI'ma sip Moscato\\r\\r\\nAnd you 'gon lose dem pants\\r\\r\\nThen I'ma throw this money\\r\\r\\nWhile you do it with no hands\\r\\r\\nGirl drop it to the flo'\\r\\r\\nI love the way yo booty go\\r\\r\\nAll I want to do is sit back\\r\\r\\nAnd watch you move\\r\\r\\nAnd I'll proceed to throw this cash\\r\\r\\n\\r\\r\\nR-O-S-C-O-E- Mr.Shawty put it on me\\r\\r\\nI be goin' ham\\r\\r\\nShawty upgrade from bologna\\r\\r\\nThem niggas tippin' good\\r\\r\\nGirl but I can make it flood\\r\\r\\n'Cause I walk around\\r\\r\\nWith pockets that are bigger than my bus\\r\\r\\nRain, rain go away\\r\\r\\nThat's what all my haters say\\r\\r\\nMy pockets stuck on overload\\r\\r\\nMy rain never evaporates\\r\\r\\nNo need to elaborate\\r\\r\\nMost of these ducks exaggerate\\r\\r\\nBut, I'ma get money nigga\\r\\r\\nEveryday stuntin' nigga\\r\\r\\nDucks might get a chance after me\\r\\r\\nBitch I'm ballin'\\r\\r\\nLike I'm comin' off of free throws\\r\\r\\n'Cause the head of the game\\r\\r\\nNo cheat codes\\r\\r\\nLambo, Roscoe\\r\\r\\nNo street code\\r\\r\\nAnd your booty got me lost like Nemo\\r\\r\\nGo, go, go\\r\\r\\nG-gon' and do yo dance\\r\\r\\nAnd, I'ma throw this money\\r\\r\\nWhile you do it wit no hands\\r\\r\\n\\r\\r\\nGirl the way you're movin'\\r\\r\\nGot me in a trance\\r\\r\\nDJ turn me up\\r\\r\\nLadies dis yo jam\\r\\r\\nI'ma sip Moscato\\r\\r\\nAnd you 'gon lose dem pants\\r\\r\\nThen I'ma throw this money\\r\\r\\nWhile you do it with no hands\\r\\r\\nGirl drop it to the flo'\\r\\r\\nI love the way your booty go\\r\\r\\nAll I want to do is sit back\\r\\r\\nAnd watch you move\\r\\r\\nAnd I'll proceed to throw this cash\""
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features #300\n",
    "train[\"reviews\"]\n",
    "train[\"reviews\"][1]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3759"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_reviews = []\n",
    "for review in train[\"reviews\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist(review,remove_stopwords=True))\n",
    "   \n",
    "\n",
    "len(clean_train_reviews)   #25000 \n",
    "######\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 3759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hejia\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 3759\n",
      "Review 2000 of 3759\n",
      "Review 3000 of 3759\n"
     ]
    }
   ],
   "source": [
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for test reviews\n",
      "Review 0 of 940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hejia\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating average feature vecs for test reviews\")    \n",
    "#######\n",
    "clean_test_reviews = []\n",
    "for review in test[\"reviews\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )  #(25000, 300)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "where_are_NaNs = isnan(testDataVecs)\n",
    "testDataVecs[where_are_NaNs] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01340841,  0.00595989,  0.01482137, -0.01143448,  0.02157097,\n",
       "        0.00666081,  0.02375888, -0.02832483,  0.03222426, -0.02747226,\n",
       "        0.00675844,  0.03860399,  0.01065748, -0.01007283,  0.03086958,\n",
       "        0.02839373,  0.00538496, -0.00528945,  0.01616366, -0.01699611,\n",
       "        0.00182417,  0.0172285 , -0.02264713,  0.00270532,  0.03890445,\n",
       "       -0.03839942, -0.01861985, -0.02068013,  0.00637237, -0.04703647,\n",
       "       -0.05112464,  0.02291659, -0.00913461, -0.0008771 ,  0.01761519,\n",
       "       -0.01009033,  0.02369462,  0.00496734,  0.02200226, -0.0268716 ,\n",
       "        0.02912227,  0.02382296,  0.04993768,  0.02264572, -0.03545407,\n",
       "       -0.00762905,  0.01159638, -0.00324208, -0.02374886, -0.04339325,\n",
       "       -0.00588375, -0.00946959, -0.01977846,  0.02112148,  0.02084436,\n",
       "        0.00900697,  0.01156769, -0.01511526, -0.03671454, -0.01650329,\n",
       "        0.01804994,  0.05074625, -0.00567346, -0.01010526,  0.00885756,\n",
       "       -0.00551338, -0.01761179,  0.00072385,  0.01333967,  0.02902069,\n",
       "        0.01398302,  0.03227495,  0.00478174, -0.01753079, -0.01600214,\n",
       "        0.013453  , -0.01178345,  0.04460155, -0.01934569,  0.00491042,\n",
       "       -0.0025291 , -0.00662787,  0.00156225,  0.00201317,  0.0183353 ,\n",
       "       -0.00554846, -0.0143219 ,  0.00564829, -0.0137838 ,  0.0364197 ,\n",
       "        0.00820012,  0.0279342 , -0.00639169,  0.01514684,  0.00082542,\n",
       "        0.00269829,  0.00267172, -0.01867446,  0.02001602, -0.01840379,\n",
       "       -0.00637059, -0.01165616, -0.0181445 , -0.00111297,  0.00405777,\n",
       "       -0.02102147, -0.00994815,  0.00068051,  0.00149643,  0.00698669,\n",
       "       -0.00027722, -0.03583875,  0.02425214, -0.01764833,  0.00266611,\n",
       "        0.0153129 , -0.02087634, -0.01504185,  0.00887663,  0.0002899 ,\n",
       "       -0.02307418, -0.03606894,  0.00984062,  0.0121602 ,  0.03297282,\n",
       "        0.03071964, -0.01686466, -0.00378076, -0.00275213,  0.01235436,\n",
       "       -0.00185231, -0.01408132,  0.00881438,  0.00416206,  0.00733878,\n",
       "       -0.02324552,  0.01498158, -0.01803361,  0.0016781 ,  0.00400392,\n",
       "       -0.02982431, -0.00385473, -0.03269441, -0.00982967,  0.0301209 ,\n",
       "       -0.01114589, -0.01596167,  0.00742311,  0.00198312,  0.00078779,\n",
       "       -0.01667422,  0.00127543,  0.02139821, -0.02849493,  0.01793772,\n",
       "       -0.00402775,  0.01963416,  0.00491117, -0.00236193, -0.0062542 ,\n",
       "       -0.00691604,  0.00823701, -0.01575448, -0.01196966, -0.01449686,\n",
       "       -0.00267413, -0.0319729 ,  0.03462993, -0.00536295, -0.02867743,\n",
       "        0.01762045, -0.00674293,  0.01036265, -0.02014979, -0.03649249,\n",
       "        0.01923444, -0.01509824, -0.0160123 ,  0.01063614, -0.01218644,\n",
       "       -0.01176035, -0.01521663,  0.00512652,  0.00837728,  0.00212443,\n",
       "       -0.0367437 ,  0.00056405, -0.01013241,  0.00509205,  0.00089331,\n",
       "        0.04157245,  0.00139844,  0.00137117, -0.01460144, -0.04514785,\n",
       "       -0.0111307 , -0.02068528,  0.0062354 ,  0.01372376,  0.02016469,\n",
       "       -0.03762981, -0.01063775,  0.03507436,  0.01653648, -0.00826621,\n",
       "        0.05917829,  0.0102967 ,  0.00207497, -0.01703642,  0.05110113,\n",
       "        0.01066804, -0.04968196, -0.02624634,  0.00883835,  0.02403715,\n",
       "        0.00653803, -0.04347511,  0.00958901, -0.01382842,  0.01438074,\n",
       "       -0.03611614, -0.02690706, -0.02504894,  0.00607205, -0.02122613,\n",
       "       -0.02439123,  0.00980572, -0.02492464, -0.02017191, -0.03112523,\n",
       "        0.00264913,  0.01807586, -0.0335187 , -0.02224077,  0.0028165 ,\n",
       "       -0.05400615, -0.03402646,  0.01467174,  0.00497636, -0.03315059,\n",
       "       -0.03288854,  0.02987604, -0.04656385, -0.01627859,  0.00144608,\n",
       "        0.00720854, -0.00035978, -0.00989272,  0.01143252,  0.03235303,\n",
       "        0.00446923, -0.04903061,  0.01533389,  0.04224454,  0.02054875,\n",
       "        0.00259144,  0.01569459,  0.01250012, -0.03247344, -0.0343166 ,\n",
       "       -0.00620992, -0.06087288, -0.00223307, -0.01446558, -0.00853064,\n",
       "       -0.026106  , -0.00177951,  0.00872613, -0.01715565, -0.00396062,\n",
       "        0.01684287,  0.00853945, -0.03629547, -0.03174238, -0.01201904,\n",
       "       -0.02282168, -0.01061734, -0.03868222, -0.00595192, -0.01431582,\n",
       "        0.00021192,  0.02607661, -0.01908765,  0.02713918,  0.01525193,\n",
       "        0.02241478,  0.05003699,  0.01708322, -0.00161689, -0.01757004,\n",
       "        0.01171987, -0.0043715 ,  0.02268231, -0.00974245, -0.01880467,\n",
       "       -0.00681002, -0.00096245, -0.04369619, -0.01564415, -0.01233261], dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDataVecs[107]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def score(classifier):\n",
    "    classifier = classifier\n",
    "    aaa = classifier.fit(trainDataVecs, train[\"sentiment\"])\n",
    "    result = aaa.predict( testDataVecs )\n",
    "    accuracy_score(test[\"sentiment\"],result)\n",
    "    return accuracy_score(test[\"sentiment\"],result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from nltk.classify.scikitlearn import  SklearnClassifier\n",
    "from sklearn.svm import SVC, LinearSVC,  NuSVC\n",
    "from sklearn.naive_bayes import  MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.metrics import  accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB`s accuracy is 0.562766\n",
      "LogisticRegression`s accuracy is  0.551064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hejia\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\hejia\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC`s accuracy is 0.500000\n",
      "LinearSVC`s accuracy is 0.558511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hejia\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NuSVC`s accuracy is 0.524468\n"
     ]
    }
   ],
   "source": [
    "print('BernoulliNB`s accuracy is %f'  %score(BernoulliNB()))\n",
    "#print('MultinomiaNB`s accuracy is %f'  %score(MultinomialNB()))\n",
    "print('LogisticRegression`s accuracy is  %f' %score(LogisticRegression()))\n",
    "print('SVC`s accuracy is %f'  %score(SVC()))\n",
    "print('LinearSVC`s accuracy is %f'  %score(LinearSVC()))\n",
    "print('NuSVC`s accuracy is %f'  %score(NuSVC()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-c36456c03f74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0maaa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sentiment\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maaa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sentiment\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#0.85011999999999999\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \"\"\"\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    298\u001b[0m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    452\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 44\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "aaa = classifier.fit(a, train[\"sentiment\"])\n",
    "result = aaa.predict( b )\n",
    "accuracy_score(test[\"sentiment\"],result)\n",
    "#0.85011999999999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'isnull'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-28b1d567467f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'isnull'"
     ]
    }
   ],
   "source": [
    "b.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 2), dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(np.isnan(trainDataVecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.10830832e-02,   5.14353719e-03,   2.52754185e-02,\n",
       "        -8.15838855e-03,   4.03351597e-02,   4.38734004e-03,\n",
       "        -3.27906683e-02,  -2.87928395e-02,   2.90968250e-02,\n",
       "         1.07309420e-03,   1.59700867e-02,  -7.32207031e-04,\n",
       "        -6.17700303e-03,   5.00782346e-03,   2.61919544e-04,\n",
       "         3.02386545e-02,  -3.43987631e-04,  -3.34602501e-03,\n",
       "         1.83834862e-02,  -1.51501903e-02,  -1.90489516e-02,\n",
       "         4.15357715e-03,   3.98910465e-03,   3.35276057e-03,\n",
       "         1.14684459e-02,  -5.69698028e-03,  -2.88710967e-02,\n",
       "         1.38502661e-02,   3.07408534e-02,  -8.53853300e-03,\n",
       "        -2.56066248e-02,   1.41214579e-02,  -7.74361053e-03,\n",
       "        -2.09821127e-02,   6.86992612e-03,  -8.40208866e-03,\n",
       "         3.06519810e-02,   2.38238424e-02,  -9.61945206e-03,\n",
       "        -1.31667415e-02,   4.94169667e-02,   6.85526896e-03,\n",
       "         4.18798439e-02,   3.78956012e-02,  -3.18664387e-02,\n",
       "        -1.90772526e-02,   3.51163186e-02,   1.15650697e-02,\n",
       "        -3.26754525e-02,  -8.92583746e-03,  -1.97916534e-02,\n",
       "         1.55542197e-03,  -4.66597937e-02,   1.49984332e-02,\n",
       "         4.01083902e-02,   2.13719271e-02,   3.38050053e-02,\n",
       "        -7.78813101e-03,  -1.52275050e-02,  -1.29999593e-02,\n",
       "         1.52636534e-02,   3.72515023e-02,  -8.03974923e-03,\n",
       "         3.50645147e-02,   5.79392631e-03,  -1.31520359e-02,\n",
       "        -2.78130472e-02,   1.46039436e-02,  -9.59383044e-03,\n",
       "         4.08869088e-02,   6.72874087e-03,   4.55144607e-03,\n",
       "        -1.27276313e-02,  -2.00070236e-02,  -1.53751709e-02,\n",
       "        -3.12120952e-02,   9.35811922e-03,   4.55326866e-03,\n",
       "         8.97215400e-03,   1.08542880e-02,   3.16753285e-03,\n",
       "        -8.71625356e-03,  -1.05859619e-02,   2.37035193e-02,\n",
       "        -4.46943846e-03,   2.04276685e-02,  -2.08610334e-02,\n",
       "         3.46290544e-02,  -1.91279352e-02,   2.36283727e-02,\n",
       "         2.19546314e-02,   5.96299535e-04,  -5.40008349e-03,\n",
       "        -1.01658171e-02,   2.09285729e-02,  -2.49608606e-02,\n",
       "         6.62308093e-03,   1.94972605e-02,   1.60859786e-02,\n",
       "         4.90728160e-03,  -4.96922527e-03,   2.92726036e-04,\n",
       "         8.79780964e-06,   7.52408011e-03,   1.82739738e-02,\n",
       "        -2.50748694e-02,  -7.88368285e-03,   1.14458362e-02,\n",
       "        -6.04228349e-04,   5.60002821e-03,  -3.80625855e-03,\n",
       "        -9.71815083e-03,   1.77348312e-02,  -2.50674747e-02,\n",
       "         1.48546789e-02,   8.45286286e-06,  -1.09522380e-02,\n",
       "        -2.71084886e-02,   2.76201647e-02,  -2.87304819e-02,\n",
       "        -2.03186907e-02,  -2.23188475e-02,   1.53024523e-02,\n",
       "         1.23675950e-02,   8.94887093e-03,   1.48978047e-02,\n",
       "        -2.03111744e-03,   1.08433450e-02,   1.86372511e-02,\n",
       "         1.37183368e-02,   2.00535879e-02,  -6.54719444e-03,\n",
       "         1.77992657e-02,   2.90609244e-03,   1.50640672e-02,\n",
       "        -2.74471976e-02,   1.28800450e-02,  -2.50124261e-02,\n",
       "        -1.51638165e-02,  -9.45043471e-03,  -1.04138330e-02,\n",
       "        -2.26312950e-02,   8.22381116e-03,   2.07042210e-02,\n",
       "         2.98655126e-02,  -2.02641618e-02,   8.04819446e-03,\n",
       "         5.88086620e-03,  -4.06260742e-03,  -8.06942116e-03,\n",
       "        -2.36481465e-02,   1.20714186e-02,   2.07357444e-02,\n",
       "        -8.41967203e-03,  -1.33923013e-02,   1.34916157e-02,\n",
       "         6.54214295e-03,   7.19680544e-03,  -8.02851375e-03,\n",
       "        -2.56887395e-02,   1.00907730e-03,  -3.50195579e-02,\n",
       "        -2.00498733e-03,   2.27553044e-02,  -1.04234619e-02,\n",
       "         5.81195485e-03,  -4.08933274e-02,   1.35191353e-02,\n",
       "         2.48810500e-02,  -8.81560892e-03,   1.58093385e-02,\n",
       "         9.24243359e-04,   4.97972919e-03,  -2.05904581e-02,\n",
       "        -5.73184853e-03,  -9.47146490e-03,  -1.13271307e-02,\n",
       "         5.94665576e-03,  -3.72331147e-03,   2.45701429e-03,\n",
       "         2.41138809e-03,   2.69451574e-03,   9.19340644e-03,\n",
       "         1.78108122e-02,  -1.55560281e-02,   8.05669930e-03,\n",
       "        -1.45183029e-02,  -3.30526964e-04,   6.83600875e-03,\n",
       "         1.20839207e-02,   2.10375171e-02,   1.43076805e-02,\n",
       "        -1.29870307e-02,  -1.93840568e-03,  -1.23588704e-02,\n",
       "        -4.42871125e-03,   2.43453924e-02,   3.85039207e-03,\n",
       "         3.75054292e-02,   5.30091173e-04,  -6.42431946e-03,\n",
       "         1.70033763e-03,   3.89871257e-03,   1.25502553e-02,\n",
       "         7.18666415e-04,   9.36669856e-03,  -6.81028760e-04,\n",
       "        -1.50721974e-03,  -3.84565517e-02,   8.83105490e-03,\n",
       "        -2.16101687e-02,  -2.14057742e-03,  -4.64314967e-02,\n",
       "         5.22852130e-03,   7.68984621e-03,  -1.37065966e-02,\n",
       "        -3.53213102e-02,   1.84267908e-02,  -1.70264707e-03,\n",
       "         2.66585033e-02,  -2.64854599e-02,  -1.15875071e-02,\n",
       "         2.92871287e-03,  -1.07460665e-02,  -3.73139931e-03,\n",
       "        -1.69710256e-02,   2.47510672e-02,   3.46309645e-03,\n",
       "        -1.12347780e-02,  -1.94741040e-02,   2.84240451e-02,\n",
       "         7.34837027e-03,   5.11730975e-03,  -1.57075915e-02,\n",
       "        -8.41431506e-03,  -8.11858010e-03,  -1.35981403e-02,\n",
       "         1.42666381e-02,   1.84159186e-02,  -3.29252891e-02,\n",
       "         5.80370845e-03,   2.38328837e-02,  -1.43976063e-02,\n",
       "        -3.06877750e-03,   3.66108987e-04,   1.17517700e-02,\n",
       "        -3.13879875e-03,   1.88779570e-02,   2.49323230e-02,\n",
       "         7.95028731e-03,   6.67621661e-03,  -2.38688923e-02,\n",
       "         3.94735709e-02,   2.71359604e-04,  -5.67606883e-04,\n",
       "         1.27716660e-02,   4.93316259e-03,   5.23164868e-03,\n",
       "        -4.44668941e-02,  -5.29227294e-02,   6.87848264e-03,\n",
       "        -5.62708192e-02,   6.82590762e-05,  -1.51501305e-03,\n",
       "         5.07408520e-03,  -1.47592593e-02,   6.56498829e-03,\n",
       "         6.29800046e-03,  -4.12141345e-03,   6.37484808e-03,\n",
       "        -1.28165111e-02,   1.77295674e-02,  -4.01045978e-02,\n",
       "        -2.86293011e-02,   2.98934225e-02,  -2.00671721e-02,\n",
       "         2.56349389e-02,  -2.40407456e-02,  -2.50041168e-02,\n",
       "         7.30498461e-04,   4.80897678e-03,   1.24293091e-02,\n",
       "        -1.67804807e-02,   2.73626707e-02,   9.10005160e-03,\n",
       "         3.67832892e-02,   1.42359491e-02,   2.30753291e-02,\n",
       "         1.77998990e-02,  -1.27287963e-02,  -1.51694687e-02,\n",
       "        -4.81479656e-05,  -5.10216877e-03,   1.38496440e-02,\n",
       "        -6.21024286e-03,  -6.33866386e-03,   1.25565415e-03,\n",
       "        -3.82164158e-02,  -1.93688571e-02,  -1.84338987e-02], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDataVecs[107]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0],\n",
       "       [   0,    1],\n",
       "       [   0,    2],\n",
       "       ..., \n",
       "       [3758,  297],\n",
       "       [3758,  298],\n",
       "       [3758,  299]], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(np.isfinite(trainDataVecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataVecs[np.isfinite(trainDataVecs)]  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.92918776e-04,   3.39077972e-02,   3.71629954e-03,\n",
       "         1.76627059e-02,   1.87370889e-02,  -1.23408856e-02,\n",
       "        -2.04783231e-02,  -1.81126859e-04,   6.25687232e-03,\n",
       "        -5.04189054e-04,   1.43532492e-02,   6.15841942e-03,\n",
       "        -3.65522504e-02,   4.67812503e-03,  -8.30571819e-03,\n",
       "         2.03098059e-02,  -3.51084443e-03,  -1.42102120e-02,\n",
       "        -7.64640979e-03,  -8.45149159e-03,   1.54138748e-02,\n",
       "        -4.86446405e-03,   1.07870176e-02,   5.92428166e-03,\n",
       "         1.70626212e-02,  -1.57537721e-02,  -1.17907636e-02,\n",
       "        -1.46874990e-02,  -1.78152812e-03,  -2.04231459e-04,\n",
       "        -1.89763438e-02,   1.97049743e-03,  -2.26746034e-02,\n",
       "        -1.31292809e-02,   5.54790022e-03,  -1.18162399e-02,\n",
       "         8.27252958e-03,   5.08068129e-03,  -4.41355212e-03,\n",
       "         4.50796587e-03,   2.71568969e-02,  -8.01084097e-03,\n",
       "         6.44361062e-05,   8.70372169e-03,  -2.72962265e-02,\n",
       "         2.97690649e-03,  -2.06444203e-03,   1.78710353e-02,\n",
       "        -8.21140409e-03,   3.66087747e-03,  -7.01495958e-03,\n",
       "        -4.94059222e-03,  -6.84454758e-03,   2.49064602e-02,\n",
       "         2.02441588e-02,   2.65961949e-04,   8.50901660e-03,\n",
       "         1.29515110e-02,  -2.42852792e-02,   1.18404543e-02,\n",
       "        -8.88883229e-03,   1.03876945e-02,   2.40646321e-02,\n",
       "         1.36276213e-02,  -1.38255237e-02,  -8.10883357e-04,\n",
       "        -3.73000503e-02,   3.58695467e-03,  -6.25005271e-03,\n",
       "        -7.04119075e-03,  -2.52273660e-02,   4.19382658e-03,\n",
       "         9.36988799e-04,  -1.41131214e-03,  -1.23010864e-02,\n",
       "        -1.17446063e-03,   6.20015990e-03,   2.21186373e-02,\n",
       "         7.84280151e-03,  -9.74390190e-03,   6.66722981e-03,\n",
       "         5.44868596e-03,   1.59185436e-02,  -8.26244615e-03,\n",
       "        -1.36357974e-02,  -4.67834482e-03,   8.80836416e-03,\n",
       "         1.13281449e-02,  -7.08192075e-03,   5.67211444e-03,\n",
       "        -1.72598902e-02,  -6.44474896e-03,   6.32957043e-03,\n",
       "        -3.78403487e-03,   4.09466634e-03,  -1.44536197e-02,\n",
       "         8.28483701e-03,   1.39013259e-03,   1.83376912e-02,\n",
       "        -4.49400675e-03,   1.25972126e-02,   6.12463709e-03,\n",
       "         9.48951766e-03,   2.73863953e-02,   1.27153425e-02,\n",
       "        -1.38208419e-02,  -8.79420061e-03,   2.46420573e-03,\n",
       "         5.61757945e-03,  -2.48439144e-02,  -4.01009806e-03,\n",
       "         9.54216905e-03,  -1.41565618e-03,  -1.40049285e-03,\n",
       "        -3.12374439e-03,   1.10395569e-02,   4.07075649e-03,\n",
       "         1.21321874e-02,   2.93583260e-03,   1.36089288e-02,\n",
       "         7.05999462e-03,   2.71042972e-03,   9.27500892e-03,\n",
       "        -1.20268967e-02,  -9.14670341e-03,  -5.22516994e-03,\n",
       "        -3.15451552e-03,   5.29975491e-03,   1.90395210e-02,\n",
       "         1.80295424e-03,   6.87737158e-03,   3.12617561e-03,\n",
       "        -5.00192726e-03,  -9.83443018e-03,  -2.23066634e-03,\n",
       "        -2.77983287e-04,   1.08128227e-02,  -3.15929530e-03,\n",
       "        -1.32859536e-02,   2.23860168e-03,  -9.08304192e-03,\n",
       "        -2.52460223e-02,   6.88552810e-03,  -2.14162320e-02,\n",
       "        -2.29806956e-02,   1.40854190e-05,  -1.23266978e-02,\n",
       "         2.61551216e-02,   2.99636996e-03,   1.29558044e-02,\n",
       "        -7.36117968e-03,   1.91318989e-02,  -4.05958388e-03,\n",
       "         5.52219478e-03,   1.63556561e-02,   5.43470867e-03,\n",
       "        -5.89563919e-04,  -6.43502863e-05,   8.38521030e-03,\n",
       "         1.93756726e-02,   1.92513447e-02,  -7.44777964e-03,\n",
       "        -5.49340388e-03,   1.68602110e-03,   6.00139704e-03,\n",
       "        -1.71681605e-02,  -1.12174992e-02,  -6.64517935e-03,\n",
       "         1.52085209e-03,  -2.11607926e-02,   1.01724989e-03,\n",
       "        -7.48144649e-03,   6.57907175e-03,  -1.32107893e-02,\n",
       "        -9.82657541e-03,  -8.41357186e-03,  -1.35709317e-02,\n",
       "        -4.14675986e-03,   4.00323141e-03,   2.31668679e-03,\n",
       "         7.75934616e-03,  -1.25771854e-02,   1.65115446e-02,\n",
       "        -9.03305691e-03,  -4.61792806e-03,  -3.39582115e-02,\n",
       "        -6.38690265e-03,  -1.07593089e-03,   3.57135683e-02,\n",
       "         2.38582189e-03,   6.70844037e-03,   1.46802273e-02,\n",
       "         5.79680083e-03,   1.21624442e-02,   2.99662724e-03,\n",
       "        -5.51171182e-03,  -5.45898732e-03,   1.49546308e-04,\n",
       "         2.44367830e-02,   8.10060557e-03,   9.26431455e-03,\n",
       "         3.52788344e-03,  -4.33534663e-03,   1.94335617e-02,\n",
       "         2.05333065e-03,   2.29224488e-02,  -1.25848232e-02,\n",
       "         1.36094997e-02,  -5.54792723e-03,  -6.49762480e-03,\n",
       "         1.96632408e-02,  -3.54590006e-02,   6.27417443e-03,\n",
       "        -2.96977558e-03,  -5.33523224e-03,  -1.62390862e-02,\n",
       "        -8.66682082e-03,   2.64628162e-03,  -1.26677249e-02,\n",
       "        -2.59615704e-02,   3.20578776e-02,   1.45679116e-02,\n",
       "         9.91653651e-03,  -1.08044688e-02,   1.08516645e-02,\n",
       "        -4.15931307e-02,   2.54148096e-02,  -2.18097344e-02,\n",
       "        -4.38117934e-03,  -1.66111384e-02,  -1.97825977e-03,\n",
       "        -1.07754786e-02,   1.60327144e-02,  -2.45442963e-03,\n",
       "         9.66349803e-03,   1.39952963e-02,   1.08104497e-02,\n",
       "         2.95569338e-02,   1.45787727e-02,  -2.72051822e-02,\n",
       "        -6.34005480e-03,  -2.66775978e-03,  -2.40029525e-02,\n",
       "        -7.48204533e-04,   3.38163739e-03,  -8.04333482e-03,\n",
       "        -5.87513298e-03,   2.76218671e-02,  -6.54831808e-03,\n",
       "         3.02917305e-02,   6.40274538e-03,  -8.41856375e-03,\n",
       "         1.29011767e-02,  -1.17742512e-02,   1.30072031e-02,\n",
       "        -1.27114421e-02,  -9.08822846e-03,   6.43585948e-03,\n",
       "        -8.76502041e-03,  -5.17039001e-03,   7.18089147e-03,\n",
       "        -3.07106990e-02,   6.83736941e-03,   2.78143748e-03,\n",
       "        -7.90525693e-03,   6.98733283e-03,   1.29700340e-02,\n",
       "         1.92830320e-02,  -1.81846414e-02,   8.35510343e-03,\n",
       "         3.51907359e-03,   6.58526178e-03,  -1.00227809e-02,\n",
       "        -1.64767820e-02,   4.64198785e-03,   2.56677046e-02,\n",
       "         5.19819837e-03,   5.47999283e-03,   8.17003846e-03,\n",
       "        -1.13740526e-02,  -2.51025874e-02,   2.05852203e-02,\n",
       "        -2.07552705e-02,   2.89546661e-02,  -1.53857265e-02,\n",
       "         1.40108243e-02,  -4.23068320e-03,   7.56800873e-03,\n",
       "         1.84495039e-02,  -1.37113798e-02,   6.45027077e-03,\n",
       "        -2.54161726e-03,  -8.24587140e-03,   6.60267193e-03,\n",
       "         4.09152638e-03,  -2.48736721e-02,  -5.04676485e-03,\n",
       "        -4.18780297e-02,  -1.48159070e-02,  -2.47903317e-02], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs[107]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(940, 300)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDataVecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
